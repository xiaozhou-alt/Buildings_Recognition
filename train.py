import numpy as np
import pandas as pd
import cv2
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import albumentations as A
from albumentations.pytorch import ToTensorV2
from tqdm import tqdm
import segmentation_models_pytorch as smp
import os
import re

# RLEç¼–è§£ç å‡½æ•° (å·²å¢å¼º)
def rle_encode(im):
    pixels = im.flatten(order='F')
    pixels = np.concatenate([[0], pixels, [0]])
    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1
    runs[1::2] -= runs[::2]
    return ' '.join(str(x) for x in runs)

def rle_decode(mask_rle, shape=(512, 512)):
    if pd.isna(mask_rle) or not isinstance(mask_rle, str) or mask_rle.strip() == '':
        return np.zeros(shape, dtype=np.uint8)
    
    # æ¸…ç†RLEç¼–ç ä¸­çš„é¢å¤–åˆ¶è¡¨ç¬¦
    mask_rle = re.sub(r'\t+', ' ', mask_rle.strip())
    
    s = mask_rle.split()
    try:
        starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]
        starts -= 1
        ends = starts + lengths
        img = np.zeros(shape[0]*shape[1], dtype=np.uint8)
        for lo, hi in zip(starts, ends):
            img[lo:hi] = 1
        return img.reshape(shape, order='F')
    except Exception as e:
        print(f"Error decoding RLE: {e}\nRLE string: '{mask_rle}'")
        return np.zeros(shape, dtype=np.uint8)

# è‡ªå®šä¹‰æ•°æ®é›†ç±»ï¼ˆå·²ä¿®å¤ï¼‰
class BuildingDataset(Dataset):
    def __init__(self, df, img_dir, transform=None, mode='train'):
        self.df = df
        self.img_dir = img_dir
        self.transform = transform
        self.mode = mode
        
        # æ¸…ç†æ–‡ä»¶å - ç§»é™¤åˆ¶è¡¨ç¬¦å’Œç©ºæ ¼
        self.df['name'] = self.df['name'].apply(lambda x: re.sub(r'\t+', '', str(x).strip()))
        
    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, idx):
        filename = self.df.iloc[idx, 0]
        # ç¡®ä¿æ–‡ä»¶åæœ‰æ­£ç¡®çš„æ‰©å±•å
        if not filename.lower().endswith(('.jpg', '.jpeg')):
            filename += '.jpg'
            
        img_path = os.path.join(self.img_dir, filename)
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(img_path):
            # å°è¯•ä¸åŒçš„å¤§å°å†™
            img_dir_files = os.listdir(self.img_dir)
            matched_files = [f for f in img_dir_files if f.lower() == filename.lower()]
            
            if matched_files:
                img_path = os.path.join(self.img_dir, matched_files[0])
                print(f"âš ï¸ ä¿®æ­£æ–‡ä»¶åå¤§å°å†™: {filename} -> {matched_files[0]}")
            else:
                print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {img_path}")
                # åˆ›å»ºæ›¿ä»£å›¾åƒ
                image = np.zeros((512, 512, 3), dtype=np.uint8)
                if self.mode == 'train' or self.mode == 'val':
                    mask = np.zeros((512, 512), dtype=np.float32)
                    if self.transform:
                        augmented = self.transform(image=image, mask=mask)
                        return augmented['image'], augmented['mask']
                    return image, mask
                else:
                    if self.transform:
                        augmented = self.transform(image=image)
                        return augmented['image']
                    return image
        
        # è¯»å–å›¾åƒ
        image = cv2.imread(img_path)
        if image is None:
            print(f"âŒ æ— æ³•è¯»å–å›¾åƒ: {img_path}")
            image = np.zeros((512, 512, 3), dtype=np.uint8)
        else:
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        if self.mode == 'train' or self.mode == 'val':
            mask_rle = self.df.iloc[idx, 1]
            mask = rle_decode(mask_rle, shape=image.shape[:2])
            mask = mask.astype(np.float32)
            
            if self.transform:
                augmented = self.transform(image=image, mask=mask)
                image = augmented['image']
                mask = augmented['mask']
            return image, mask
        
        else:  # æµ‹è¯•æ¨¡å¼
            if self.transform:
                augmented = self.transform(image=image)
                image = augmented['image']
            return image

# æ•°æ®å¢å¼ºé…ç½®
def get_train_transform():
    return A.Compose([
        A.Resize(384, 384),
        A.HorizontalFlip(p=0.5),
        A.VerticalFlip(p=0.5),
        A.RandomRotate90(p=0.5),
        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),
        A.RandomBrightnessContrast(p=0.2),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2()
    ])

def get_val_transform():
    return A.Compose([
        A.Resize(384, 384),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2()
    ])

# Diceç³»æ•°è®¡ç®—å‡½æ•°
def dice_coeff(pred, target, smooth=1e-6):
    intersection = (pred * target).sum()
    return (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)

# æ¨¡å‹è®­ç»ƒå‡½æ•°ï¼ˆæ·»åŠ æ—©åœæœºåˆ¶ï¼‰
def train_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=15, patience=5):
    best_val_dice = 0.0
    best_epoch = 0
    history = {'train_loss': [], 'val_dice': []}
    no_improve_count = 0  # è®°å½•æ²¡æœ‰æå‡çš„epochæ•°
    
    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        
        # è®­ç»ƒé˜¶æ®µ
        for batch_idx, (images, masks) in enumerate(tqdm(train_loader, 
                                                      desc=f"Epoch {epoch+1}/{num_epochs} - Training",
                                                      total=len(train_loader))):
            images = images.to(device)
            masks = masks.to(device).unsqueeze(1)
            
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, masks)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item() * images.size(0)
            
            # æ¯100ä¸ªbatchæ‰“å°ä¸€æ¬¡è¿›åº¦
            if batch_idx % 100 == 0:
                print(f"Batch {batch_idx}/{len(train_loader)} - Loss: {loss.item():.4f}")
        
        epoch_loss = running_loss / len(train_loader.dataset)
        history['train_loss'].append(epoch_loss)
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_dice = 0.0
        with torch.no_grad():
            for images, masks in tqdm(val_loader, desc="Validating", total=len(val_loader)):
                images = images.to(device)
                masks = masks.to(device).unsqueeze(1)
                
                outputs = model(images)
                preds = torch.sigmoid(outputs) > 0.5
                val_dice += dice_coeff(preds.float(), masks).item()
        
        val_dice /= len(val_loader)
        history['val_dice'].append(val_dice)
        
        print(f"Epoch {epoch+1}/{num_epochs} | Train Loss: {epoch_loss:.4f} | Val Dice: {val_dice:.4f}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æå‡
        if val_dice > best_val_dice:
            best_val_dice = val_dice
            best_epoch = epoch
            no_improve_count = 0
            torch.save(model.state_dict(), '/kaggle/working/best_model.pth')
            print(f"ğŸ’¾ ä¿å­˜æ–°çš„æœ€ä½³æ¨¡å‹ï¼ŒDice: {best_val_dice:.4f}")
        else:
            no_improve_count += 1
            print(f"ğŸ”„ éªŒè¯é›†Diceæœªæå‡ï¼Œè¿ç»­ {no_improve_count}/{patience} ä¸ªepoch")
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ—©åœæ¡ä»¶
            if no_improve_count >= patience:
                print(f"ğŸ›‘ æ—©åœè§¦å‘ï¼è¿ç»­ {patience} ä¸ªepochéªŒè¯é›†Diceæœªæå‡ã€‚")
                print(f"ğŸ† æœ€ä½³éªŒè¯é›†Dice: {best_val_dice:.4f} (epoch {best_epoch+1})")
                break
    
    # å¦‚æœæ­£å¸¸å®Œæˆæ‰€æœ‰epochï¼Œæ‰“å°æœ€ä½³ç»“æœ
    if no_improve_count < patience:
        print(f"âœ… è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯é›†Dice: {best_val_dice:.4f} (epoch {best_epoch+1})")
    
    return history

# ä¸»ç¨‹åº
def main():
    # é…ç½®å‚æ•°
    BATCH_SIZE = 8
    LR = 0.001
    EPOCHS = 50
    PATIENCE = 5  # æ—©åœè€å¿ƒå€¼
    IMG_SIZE = 384
    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    print("="*50)
    print(f"ğŸš€ å¯åŠ¨å»ºç­‘ç‰©åˆ†å‰²è®­ç»ƒ")
    print(f"ğŸ“Œ è®¾å¤‡: {DEVICE}")
    print(f"ğŸ“Š æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    print(f"ğŸ¯ å­¦ä¹ ç‡: {LR}")
    print(f"ğŸ”„ è®­ç»ƒè½®æ•°: {EPOCHS}")
    print(f"â±ï¸ æ—©åœè€å¿ƒå€¼: {PATIENCE}")
    print("="*50)
    
    # åŠ è½½æ•°æ® - ä½¿ç”¨åˆ¶è¡¨ç¬¦åˆ†éš”ï¼Œå¤„ç†ç©ºå€¼
    print("ğŸ“‚ åŠ è½½æ•°æ®...")
    train_df = pd.read_csv(
        '/kaggle/input/buildings-recognition-processed/train.csv', 
        header=None, 
        sep='\t',  # ä½¿ç”¨åˆ¶è¡¨ç¬¦åˆ†éš”
        names=['name', 'mask'],
        on_bad_lines='warn'  # å¿½ç•¥æ ¼å¼ä¸æ­£ç¡®çš„è¡Œ
    )
    test_df = pd.read_csv(
        '/kaggle/input/buildings-recognition-processed/test.csv', 
        header=None, 
        sep='\t',  # ä½¿ç”¨åˆ¶è¡¨ç¬¦åˆ†éš”
        names=['name', 'mask'],
        on_bad_lines='warn'
    )
    
    # å¤„ç†ç©ºå€¼ - å¯¹äºæ²¡æœ‰å»ºç­‘ç‰©çš„å›¾åƒï¼Œmaskä¸ºç©º
    train_df['mask'] = train_df['mask'].fillna('')
    test_df['mask'] = test_df['mask'].fillna('')
    
    # æ¸…ç†æ•°æ® - ç§»é™¤æ–‡ä»¶åä¸­çš„åˆ¶è¡¨ç¬¦
    train_df['name'] = train_df['name'].apply(lambda x: re.sub(r'\t+', '', str(x).strip()))
    test_df['name'] = test_df['name'].apply(lambda x: re.sub(r'\t+', '', str(x).strip()))
    
    # æ‰“å°æ ·æœ¬
    print("\nè®­ç»ƒæ•°æ®æ ·æœ¬:")
    print(train_df.head())
    print(f"\nè®­ç»ƒé›†å¤§å°: {len(train_df)}")
    print(f"æ²¡æœ‰å»ºç­‘ç‰©çš„å›¾åƒæ•°é‡: {train_df['mask'].eq('').sum()}")
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)
    print(f"è®­ç»ƒé›†: {len(train_df)}, éªŒè¯é›†: {len(val_df)}")
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    train_dataset = BuildingDataset(
        train_df, 
        img_dir='/kaggle/input/buildings-recognition-processed/train', 
        transform=get_train_transform(), 
        mode='train'
    )
    val_dataset = BuildingDataset(
        val_df, 
        img_dir='/kaggle/input/buildings-recognition-processed/train', 
        transform=get_val_transform(), 
        mode='val'
    )
    test_dataset = BuildingDataset(
        test_df, 
        img_dir='/kaggle/input/buildings-recognition-processed/test', 
        transform=get_val_transform(), 
        mode='test'
    )
    
    # éªŒè¯æ–‡ä»¶è·¯å¾„
    sample_idx = 0
    sample_file = os.path.join('/kaggle/input/buildings-recognition-processed/train', train_df.iloc[sample_idx, 0])
    print(f"\néªŒè¯æ–‡ä»¶è·¯å¾„: {sample_file}")
    print(f"æ–‡ä»¶å­˜åœ¨: {os.path.exists(sample_file)}")
    if os.path.exists(sample_file):
        print(f"æ–‡ä»¶å¤§å°: {os.path.getsize(sample_file)} bytes")
    
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)
    
    # åˆå§‹åŒ–æ¨¡å‹
    print("\nğŸ›  åˆå§‹åŒ–æ¨¡å‹...")
    model = smp.Unet(
        encoder_name='efficientnet-b4',
        encoder_weights='imagenet',
        in_channels=3,
        classes=1,
        activation=None
    ).to(DEVICE)
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    print(f"æ¨¡å‹æ¶æ„: U-Net with EfficientNet-B4")
    total_params = sum(p.numel() for p in model.parameters())
    print(f"æ€»å‚æ•°é‡: {total_params/1e6:.2f}M")
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)
    optimizer = optim.Adam(model.parameters(), lr=LR)
    
    # è®­ç»ƒæ¨¡å‹
    print("\nğŸ”¥ å¼€å§‹è®­ç»ƒ...")
    history = train_model(
        model, train_loader, val_loader,
        optimizer, criterion, DEVICE, 
        num_epochs=EPOCHS, patience=PATIENCE
    )
    
    # æµ‹è¯•é›†è¯„ä¼°
    print("\nğŸ§ª æµ‹è¯•é›†è¯„ä¼°...")
    model.load_state_dict(torch.load('/kaggle/working/best_model.pth'))
    model.eval()
    
    test_dice = 0.0
    with torch.no_grad():
        for images, masks in tqdm(val_loader, desc="æµ‹è¯•ä¸­", total=len(val_loader)):
            images = images.to(DEVICE)
            masks = masks.to(DEVICE).unsqueeze(1)
            
            outputs = model(images)
            preds = torch.sigmoid(outputs) > 0.5
            test_dice += dice_coeff(preds.float(), masks).item()
    
    test_dice /= len(val_loader)
    print(f"âœ… æµ‹è¯•é›†Diceç³»æ•°: {test_dice:.4f}")
    
    # å¯è§†åŒ–5ä¸ªæµ‹è¯•æ ·æœ¬
    print("\nğŸ¨ å¯è§†åŒ–é¢„æµ‹ç»“æœ...")
    model.eval()
    with torch.no_grad():
        fig, axes = plt.subplots(5, 3, figsize=(15, 25))
        
        # ç¡®ä¿æµ‹è¯•é›†æœ‰è¶³å¤Ÿæ ·æœ¬
        if len(test_dataset) < 5:
            sample_indices = range(len(test_dataset))
        else:
            sample_indices = np.random.choice(len(test_dataset), 5, replace=False)
        
        for i, idx in enumerate(sample_indices):
            image = test_dataset[idx]
            
            if isinstance(image, tuple):  # å¦‚æœæµ‹è¯•é›†åŒ…å«mask
                image_tensor, true_mask = image
                true_mask = true_mask.cpu().numpy()
            else:
                image_tensor = image
                true_mask = None
            
            # é¢„æµ‹
            input_tensor = image_tensor.unsqueeze(0).to(DEVICE)
            output = model(input_tensor)
            pred_mask = (torch.sigmoid(output) > 0.5).squeeze().cpu().numpy()
            
            # åæ ‡å‡†åŒ–å›¾åƒ
            image_np = image_tensor.permute(1, 2, 0).cpu().numpy()
            image_np = image_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])
            image_np = np.clip(image_np, 0, 1)
            
            # æ˜¾ç¤ºç»“æœ
            axes[i, 0].imshow(image_np)
            axes[i, 0].set_title(f'origin: {test_df.iloc[idx, 0]}')
            axes[i, 0].axis('off')
            
            if true_mask is not None:
                axes[i, 1].imshow(true_mask, cmap='gray')
                axes[i, 1].set_title('true_mask')
            else:
                axes[i, 1].axis('off')
                axes[i, 1].set_title('no_mask')
            
            axes[i, 2].imshow(pred_mask, cmap='gray')
            axes[i, 2].set_title('predict_mask')
            axes[i, 2].axis('off')
        
        plt.tight_layout()
        plt.savefig('/kaggle/working/predictions.png')
        plt.show()
        print("ğŸ“¸ é¢„æµ‹ç»“æœå·²ä¿å­˜ä¸º /kaggle/working/predictions.png")

if __name__ == '__main__':
    main()